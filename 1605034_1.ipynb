{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# line by line file reader\n",
    "\n",
    "file = open(\"trainLinearlyNonSeparable.txt\", \"r\")\n",
    "\n",
    "# read the first line\n",
    "line = file.readline()\n",
    "\n",
    "# extract the numbers from the line as integers\n",
    "numbers = [int(x) for x in line.split()]\n",
    "\n",
    "number_of_features = numbers[0]\n",
    "number_of_class = numbers[1]\n",
    "number_of_samples = numbers[2]\n",
    "\n",
    "X_train = []\n",
    "\n",
    "for i in range(number_of_samples):\n",
    "    line = file.readline()\n",
    "    numbers = [float(x) for x in line.split()]\n",
    "    X_train.append(numbers)\n",
    "\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "y_train = X_train[:, number_of_features]\n",
    "X_train = X_train[:, :number_of_features]\n",
    "\n",
    "# convert labels to -1 and 1\n",
    "y_train[y_train == 2] = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pocket algorithm for perceptron algorithm\n",
    "\n",
    "def perceptron_pocket(X,y,max_iter=1000) :\n",
    "    # add bias to X\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "    # initialize the weight vector\n",
    "    w = np.zeros(number_of_features + 1)\n",
    "    w_best = np.zeros(number_of_features + 1)\n",
    "    p = 0.01\n",
    "\n",
    "    h_best = np.inf\n",
    "\n",
    "    for iter in range(max_iter):\n",
    "        misclassified = []\n",
    "        # loop over the samples\n",
    "        for i in range(number_of_samples):\n",
    "            # compute the prediction\n",
    "            prediction = np.dot(X[i], w)\n",
    "\n",
    "            if y[i] == -1 and prediction <= 0:\n",
    "                misclassified.append(i)\n",
    "            elif y[i] == 1 and prediction > 0:\n",
    "                misclassified.append(i)\n",
    "\n",
    "        # loop over the misclassified samples\n",
    "        for i in misclassified:\n",
    "            # update the weight vector\n",
    "            w = w - p * y[i] * X[i]\n",
    "        \n",
    "        \n",
    "        if len(misclassified) < h_best:\n",
    "            h_best = len(misclassified)\n",
    "            w_best = w\n",
    "\n",
    "        #print(iter, len(misclassified))\n",
    "    \n",
    "    return w_best\n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "# test the algorithm\n",
    "\n",
    "weights = perceptron_pocket(X_train, y_train)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample no \tfeature values \t\t\tactual class \tpredicted class\n",
      "170 \t [1.         1.60866581 3.86437026 5.29833973 8.01541067] \t\t\t -1.0 \t 1.0\n",
      "171 \t [1.         2.03037464 4.63482989 6.94098711 8.53590213] \t\t\t -1.0 \t 1.0\n",
      "172 \t [ 1.          3.99237585  8.08090113 12.34340583 16.05956163] \t\t\t 1.0 \t -1.0\n",
      "173 \t [ 1.          4.15975357  7.65456696 11.77586291 16.58566104] \t\t\t 1.0 \t -1.0\n",
      "174 \t [1.         2.14033335 3.66825436 6.18842916 8.16284464] \t\t\t -1.0 \t 1.0\n",
      "175 \t [ 1.          4.3124503   8.08462318 11.48604125 15.18687984] \t\t\t 1.0 \t -1.0\n",
      "176 \t [ 1.          4.57722904  8.04661657 11.29743912 15.80765912] \t\t\t 1.0 \t -1.0\n",
      "177 \t [ 1.          3.3351582   8.35192192 12.63921605 15.63565217] \t\t\t 1.0 \t -1.0\n",
      "178 \t [1.         2.06832699 4.64714954 6.33482759 8.0155349 ] \t\t\t -1.0 \t 1.0\n",
      "Accuracy:  0.9775\n"
     ]
    }
   ],
   "source": [
    "file = open(\"testLinearlyNonSeparable.txt\", \"r\")\n",
    "\n",
    "X_test = []\n",
    "\n",
    "for i in range(number_of_samples):\n",
    "    line = file.readline()\n",
    "    numbers = [float(x) for x in line.split()]\n",
    "    X_test.append(numbers)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "y_test = X_test[:, number_of_features]\n",
    "X_test = X_test[:, :number_of_features]\n",
    "\n",
    "# convert labels to -1 and 1\n",
    "y_test[y_test == 2] = -1\n",
    "\n",
    "# add bias to X_test\n",
    "X_test = np.insert(X_test, 0, 1, axis=1)\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "weights = perceptron_pocket(X_train, y_train)\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# compute the prediction\n",
    "prediction = np.dot(X_test, weights)\n",
    "\n",
    "\n",
    "\n",
    "print('sample no \\tfeature values \\t\\t\\tactual class \\tpredicted class')\n",
    "\n",
    "# accuracy mesurement\n",
    "\n",
    "for i in range(number_of_samples):\n",
    "    if prediction[i] <= 0:\n",
    "        prediction[i] = 1\n",
    "    else:\n",
    "        prediction[i] = -1\n",
    "\n",
    "correct = 0\n",
    "for i in range(number_of_samples):\n",
    "    if prediction[i] ==  y_test[i]:\n",
    "        correct += 1\n",
    "    else:\n",
    "        print(i, '\\t', X_test[i], '\\t\\t\\t', y_test[i], '\\t', prediction[i])\n",
    "    \n",
    "   \n",
    "\n",
    "accuracy = correct / number_of_samples\n",
    "\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a6f8c3de90b578c42c5dca1ace0652505f65d16b2ece0eaf147a9cf03f55f9e2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
